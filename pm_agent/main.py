import sys
from pathlib import Path

# Add project root to path
sys.path.append(str(Path(__file__).resolve().parent.parent))

from pm_agent.safe_executor import (
    execute_pandas_code,
    get_df_info_for_llm,
    validate_code_syntax,
)
from pm_agent.rag import RAGManager
from pm_agent.llm import LLMClient
from pm_agent.data_processor import DataProcessor  # Used for loading only
from pm_agent.config import MISTRAL_API_KEY  # noqa: F401
from pm_agent.chat_tools import execute_tool, get_tools_description
from pm_agent.agents.visualization import VisualizationAgent
from pm_agent.agents.report import ReportAgent
from pm_agent.agents.profiling import DataProfilingAgent
from pm_agent.agents.discovery import ProcessDiscoveryAgent
from pm_agent.agents.cleaning import DataCleaningAgent
from pm_agent.agents.analysis import ProcessAnalysisAgent
import glob
import argparse
import datetime
import json
import os
import time

import pandas as pd

# Force UTF-8 for console output
sys.stdout.reconfigure(encoding="utf-8")


# Import Agents


def safe_input(prompt: str = "") -> str:
    """
    Safely read user input with Unicode error handling.
    Handles UnicodeDecodeError that can occur when special keys (like backspace)
    produce invalid UTF-8 byte sequences in certain terminal environments.
    Invalid bytes are automatically filtered out without requiring re-entry.
    """
    try:
        # Read raw bytes from stdin to handle potential encoding issues
        sys.stdout.write(prompt)
        sys.stdout.flush()

        if hasattr(sys.stdin, "buffer"):
            # Read raw bytes and decode with error handling
            raw_bytes = sys.stdin.buffer.readline()
            # Decode with 'ignore' to silently drop invalid bytes
            user_input = raw_bytes.decode("utf-8", errors="ignore").rstrip("\n\r")
        else:
            # Fallback for environments without buffer access
            user_input = input()

        return user_input.strip()
    except EOFError:
        # Handle Ctrl+D
        return "exit"
    except KeyboardInterrupt:
        # Handle Ctrl+C
        print()
        return "exit"
    except Exception:
        try:
            return input().encode("utf-8", errors="replace").decode("utf-8").strip()
        except Exception:
            return ""


def find_latest_session(base_filename: str) -> str | None:
    """Finds the latest session directory for the given filename."""
    search_pattern = os.path.join("reports", f"{base_filename}_*")
    dirs = glob.glob(search_pattern)
    if not dirs:
        return None
    # Sort by creation time (name contains timestamp, so lexicographical sort works too if format is YYYYMMDD_HHMMSS)
    dirs.sort(reverse=True)
    return dirs[0]


def run_tool_wrapper(tool_name: str, agent_func, llm: LLMClient, **kwargs):
    """
    Executes a step (tool) with infinite retry logic (capped at 15) using feedback.
    """
    result = None
    feedback = ""
    result_str = ""
    MAX_SAFE_RETRIES = 15

    for attempt in range(1, MAX_SAFE_RETRIES + 1):
        try:
            # Try passing feedback if available
            if feedback:
                print(f"   üîÑ –ü–µ—Ä–µ–¥–∞—á–∞ –∫—Ä–∏—Ç–∏–∫–∏ –∞–≥–µ–Ω—Ç—É: {feedback[:100]}...")
                try:
                    # Optimistically pass feedback
                    result = agent_func(feedback=feedback, **kwargs)
                except TypeError:
                    # Fallback for agents that don't support feedback yet
                    result = agent_func(**kwargs)
            else:
                result = agent_func(**kwargs)

            # Ensure result_str is string for Judge
            if isinstance(result, tuple):
                result_str = result[0]
            else:
                result_str = (
                    result
                    if isinstance(result, str)
                    else json.dumps(result, indent=2, ensure_ascii=False)
                )

        except Exception as e:
            result_str = str(f"–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {e}")
            print(result_str)

        # Judge the result
        judge_res = llm.judge_step(tool_name, f"–ü–æ–ø—ã—Ç–∫–∞ {attempt}", result_str)
        passed = judge_res.get("passed", True)
        critique = judge_res.get("critique", "–ù–µ—Ç –∑–∞–º–µ—á–∞–Ω–∏–π")
        score = judge_res.get("score", 5)

        # Display thoughts (briefly)
        try:
            res_data = json.loads(result_str)
            if isinstance(res_data, dict) and "thoughts" in res_data:
                print(f"   üí° Thoughts: {res_data['thoughts']}")
        except (json.JSONDecodeError, TypeError):
            pass

        print(
            f"   >>> –°—É–¥—å—è (–ü–æ–ø—ã—Ç–∫–∞ {attempt}/{MAX_SAFE_RETRIES}): {'–ü–†–ò–ù–Ø–¢–û' if passed else '–û–¢–ö–õ–û–ù–ï–ù–û'} (–û—Ü–µ–Ω–∫–∞: {score})."
        )
        if not passed:
            print(f"   üìù –ö—Ä–∏—Ç–∏–∫–∞: {critique}")

        if passed or attempt == MAX_SAFE_RETRIES:
            if attempt == MAX_SAFE_RETRIES and not passed:
                print(
                    f"   ‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ: –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –ø–æ–ø—ã—Ç–æ–∫ ({MAX_SAFE_RETRIES}). –ü—Ä–∏–Ω–∏–º–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç '–∫–∞–∫ –µ—Å—Ç—å'."
                )
            return result

        feedback = critique

    return result_str


def print_used_tools(used_tools: set):
    """Prints a summary of all tools used during the session."""
    if not used_tools:
        return
    print("\n=========================================")
    print("üõ†Ô∏è –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ù–´–ï –ò–ù–°–¢–†–£–ú–ï–ù–¢–´")
    for i, tool in enumerate(sorted(used_tools), 1):
        print(f"{i}. {tool}")
    print("=========================================\n")


def main():
    start_time = time.time()
    used_tools = set()
    parser = argparse.ArgumentParser(description="Process Mining AI Agent (ReAct)")
    parser.add_argument(
        "--file", type=str, required=True, help="Path to the log file (CSV)"
    )
    parser.add_argument(
        "--rag-file", type=str, help="Path to the Excel file for RAG (optional)"
    )
    args = parser.parse_args()

    # Setup Output
    input_filename = Path(args.file).stem

    # Check for previous session
    latest_session = find_latest_session(input_filename)
    resume_mode = False
    output_dir = ""

    # Init RAG
    rag_manager = None
    if args.rag_file:
        rag_manager = RAGManager()
        rag_manager.load_excel(args.rag_file)

    # Init Core
    llm = LLMClient(rag_manager=rag_manager)
    artifacts = {}
    memory = ""
    current_df = None
    chat_history = []

    # Default Knowledge Base Content
    default_kb_content = (
        "# Knowledge Base & Glossary\n\n"
        "## Definitions (User Defaults)\n"
        "- **–≠–∫–∑–µ–º–ø–ª—è—Ä (Case)**: –≠—Ç–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –¥–ª—è –Ω–∞–±–æ—Ä–∞ —Å–æ–±—ã—Ç–∏–π. –í –æ–¥–Ω–æ–º —ç–∫–∑–µ–º–ø–ª—è—Ä–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –º–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ (–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–µ–π).\n"
        "- **–ü—Ä–æ—Ü–µ—Å—Å (Activity)**: –ö–∞–∫–∞—è-—Ç–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å/–¥–µ–π—Å—Ç–≤–∏–µ. –û–¥–Ω–∞ –∏ —Ç–∞ –∂–µ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–æ–∂–µ—Ç –≤—Å—Ç—Ä–µ—á–∞—Ç—å—Å—è –≤ —Ä–∞–∑–Ω—ã—Ö —ç–∫–∑–µ–º–ø–ª—è—Ä–∞—Ö.\n\n"
        "## User Insights\n"
    )

    if latest_session:
        print(f"\nüîç –ù–∞–π–¥–µ–Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–∞—è —Å–µ—Å—Å–∏—è: {latest_session}")
        choice = safe_input("–í–æ–∑–æ–±–Ω–æ–≤–∏—Ç—å —Ä–∞–±–æ—Ç—É —Å –Ω–µ–π? (y/n): ").lower()
        if choice == "y":
            resume_mode = True
            output_dir = latest_session
            print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏–∑ {output_dir}...")

            # Load Memory
            try:
                with open(
                    os.path.join(output_dir, "memory.md"), "r", encoding="utf-8"
                ) as f:
                    memory = f.read()
                print("‚úÖ –ü–∞–º—è—Ç—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")
            except FileNotFoundError:
                print("‚ö†Ô∏è memory.md –Ω–µ –Ω–∞–π–¥–µ–Ω. –ù–∞—á–∏–Ω–∞–µ–º —Å –ø—É—Å—Ç–æ–π –ø–∞–º—è—Ç–∏.")
                memory = "Resumed session. Memory file missing."

            # Load Final Report (for Context)
            try:
                with open(
                    os.path.join(output_dir, "final_report.md"), "r", encoding="utf-8"
                ) as f:
                    final_report_content = f.read()
                print("‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω.")
            except FileNotFoundError:
                final_report_content = "Report missing."

            # Load Chat History
            chat_json_path = os.path.join(output_dir, "chat_history.json")
            if os.path.exists(chat_json_path):
                try:
                    with open(chat_json_path, "r", encoding="utf-8") as f:
                        chat_history = json.load(f)
                    print(f"‚úÖ –ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ ({len(chat_history)} —Å–æ–æ–±—â–µ–Ω–∏–π).")
                except Exception:
                    print("‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —á–∞—Ç–∞.")

            # Check/Create Knowledge Base
            kb_path = os.path.join(output_dir, "knowledge_base.md")
            if not os.path.exists(kb_path):
                print("‚ö†Ô∏è –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, —Å–æ–∑–¥–∞—é –Ω–æ–≤—É—é...")
                with open(kb_path, "w", encoding="utf-8") as f:
                    f.write(default_kb_content)

    if not resume_mode:
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = os.path.join("reports", f"{input_filename}_{timestamp}")
        os.makedirs(output_dir, exist_ok=True)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {output_dir}")
        memory = "Session started. Data loaded successfully. No analysis steps performed yet."

        # 1. Load Data
        loader = DataProcessor(args.file)
        success, msg = loader.load_data()
        if not success:
            print(f"Critical Error: {msg}")
            print_used_tools(used_tools)
            return
        df = loader.df
        print(f"–î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –°—Ç—Ä–æ–∫: {len(df)}")
        current_df = df.copy()  # Working copy

        # Initialize Knowledge Base for new session
        kb_path = os.path.join(output_dir, "knowledge_base.md")
        with open(kb_path, "w", encoding="utf-8") as f:
            f.write(default_kb_content)

    else:
        # Load DataFrame for resume mode (needed for tools)
        df_path = os.path.join(output_dir, "dataframe.pkl")
        if os.path.exists(df_path):
            current_df = pd.read_pickle(df_path)
            print(f"‚úÖ DataFrame –∑–∞–≥—Ä—É–∂–µ–Ω ({len(current_df)} —Å—Ç—Ä–æ–∫).")
        else:
            current_df = None
            print("‚ö†Ô∏è DataFrame –Ω–µ –Ω–∞–π–¥–µ–Ω. –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã.")

    # Tools definition
    tools_desc = """
    1. Data Profiling: –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∏, —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–æ–ø—É—Å–∫–∏. –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ Case ID, Activity, Timestamp.
    2. Data Cleaning: –û—á–∏—â–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ (–ø—Ä–æ–ø—É—Å–∫–∏, —Ç–∏–ø—ã) –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è. –¢–†–ï–ë–£–ï–¢ Data Profiling.
    3. Process Discovery: –°—Ç—Ä–æ–∏—Ç DFG –≥—Ä–∞—Ñ –ø—Ä–æ—Ü–µ—Å—Å–∞ –∏ Mermaid —Å—Ö–µ–º—É. –¢–†–ï–ë–£–ï–¢ –æ—á–∏—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
    4. Visualization: –°—Ç—Ä–æ–∏—Ç –≥—Ä–∞—Ñ–∏–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–æ–±—ã—Ç–∏–π –∏ –≤—Ä–µ–º–µ–Ω–∏.
    5. Process Analysis: –°—á–∏—Ç–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –∏—â–µ—Ç —É–∑–∫–∏–µ –º–µ—Å—Ç–∞ (bottlenecks). –¢–†–ï–ë–£–ï–¢ Process Discovery (PM columns).
    6. Reporting: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç Markdown. –¢–†–ï–ë–£–ï–¢ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –æ—Å–Ω–æ–≤–Ω—ã—Ö —à–∞–≥–æ–≤ –∞–Ω–∞–ª–∏–∑–∞.
    7. Finish: –ó–∞–≤–µ—Ä—à–∞–µ—Ç —Ä–∞–±–æ—Ç—É. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¢–û–õ–¨–ö–û –ø–æ—Å–ª–µ Reporting.
    """

    MAX_STEPS = 10

    # --- Session Loop (Global Retry) ---
    # --- Session Loop (Global Retry) ---
    MAX_SESSION_RETRIES = 2  # 1 initial + 1 retry
    session_attempt = 0
    # If resuming, we assume the previous session was passed or user wants to chat anyway
    global_passed = True if resume_mode else False

    # Only run analysis loop if NOT resuming
    if not resume_mode:
        while session_attempt < MAX_SESSION_RETRIES and not global_passed:
            session_attempt += 1
            print("=========================================")
            print(f"üöÄ –ó–ê–ü–£–°–ö –°–ï–°–°–ò–ò (–ü–æ–ø—ã—Ç–∫–∞ {session_attempt}/{MAX_SESSION_RETRIES})")
            print("=========================================\n")

            step_count = 0
            final_report_content = ""
            cumulative_outputs = []

            while step_count < MAX_STEPS:
                step_count += 1
                print(f"\n--- –†–∞—É–Ω–¥ {step_count} (–°–µ—Å—Å–∏—è {session_attempt}) ---")

                # 1. Decide (using Memory)
                decision = llm.decide_next_step(memory, tools_desc)

                thought = decision.get("thought", "No thought")
                tool_name = decision.get("tool_name", "Unknown")

                print(f"üí≠ –ú–´–°–õ–¨ –ê–ì–ï–ù–¢–ê: {thought}")
                print(f"üëâ –í–´–ë–û–† –ò–ù–°–¢–†–£–ú–ï–ù–¢–ê: {tool_name}")

                if tool_name == "Finish":
                    used_tools.add(tool_name)
                    print("üèÅ –ê–≥–µ–Ω—Ç –∑–∞–ø—Ä–æ—Å–∏–ª –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.")
                    break

                used_tools.add(tool_name)
                # 2. Execute
                current_step_result_str = ""

                try:
                    if tool_name == "Data Profiling":
                        agent = DataProfilingAgent(current_df.copy(), llm)
                        res_json = run_tool_wrapper(tool_name, agent.run, llm)
                        artifacts["profiling"] = json.loads(res_json)
                        current_step_result_str = (
                            f"Data Profiling completed. Readiness: "
                            f"{artifacts['profiling'].get('process_mining_readiness', {}).get('level')}"
                        )

                    elif tool_name == "Data Cleaning":
                        if "profiling" not in artifacts:
                            raise ValueError("Requires Profiling first")
                        agent = DataCleaningAgent(current_df.copy(), llm)
                        # Cleaning returns tuple (report, new_df)
                        res = run_tool_wrapper(
                            tool_name,
                            agent.run,
                            llm,
                            profiling_report=artifacts["profiling"],
                        )

                        clean_report_json = ""
                        if isinstance(res, tuple):
                            clean_report_json, cleaned_df = res
                            current_df = cleaned_df  # Update working DF
                        else:
                            clean_report_json = res

                        artifacts["cleaning"] = json.loads(clean_report_json)
                        current_step_result_str = (
                            "Data Cleaning completed. DataFrame updated."
                        )

                    elif tool_name == "Visualization":
                        if "profiling" not in artifacts:
                            raise ValueError(
                                "Requires Profiling first for column detection"
                            )
                        agent = VisualizationAgent(current_df.copy(), llm)
                        res_json = run_tool_wrapper(
                            tool_name,
                            agent.run,
                            llm,
                            profiling_report=artifacts["profiling"],
                            output_dir=output_dir,
                        )
                        artifacts["visualization"] = json.loads(res_json)
                        current_step_result_str = (
                            "Visualization completed. 4 charts generated."
                        )

                    elif tool_name == "Process Discovery":
                        agent = ProcessDiscoveryAgent(current_df.copy(), llm)
                        # Pass output_dir
                        res_json = run_tool_wrapper(
                            tool_name,
                            agent.run,
                            llm,
                            pm_columns=None,
                            output_dir=output_dir,
                        )
                        artifacts["discovery"] = json.loads(res_json)
                        current_step_result_str = f"Process Discovery completed. Found {artifacts['discovery'].get('activities')} activities."

                    elif tool_name == "Process Analysis":
                        if "discovery" not in artifacts:
                            raise ValueError("Requires Discovery first (PM columns)")
                        confirmed_pm_cols = artifacts["discovery"].get("pm_columns")
                        agent = ProcessAnalysisAgent(current_df.copy(), llm)
                        res_json = run_tool_wrapper(
                            tool_name,
                            agent.run,
                            llm,
                            pm_columns=confirmed_pm_cols,
                            output_dir=output_dir,
                        )
                        artifacts["analysis"] = json.loads(res_json)
                        current_step_result_str = "Process Analysis completed. Performance metrics calculated."

                    elif tool_name == "Reporting":
                        if not artifacts:
                            current_step_result_str = (
                                "Reporting failed: No artifacts to report."
                            )
                        else:
                            agent = ReportAgent(llm)
                            final_report_content = run_tool_wrapper(
                                tool_name, agent.run, llm, artifacts=artifacts
                            )

                            report_path = os.path.join(output_dir, "final_report.md")
                            with open(report_path, "w", encoding="utf-8") as f:
                                f.write(final_report_content)
                            print(f"üìÑ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")
                            current_step_result_str = f"Reporting completed. Final report saved to {report_path}."

                            # Save DataFrame for future resume/tools
                            df_path = os.path.join(output_dir, "dataframe.pkl")
                            current_df.to_pickle(df_path)
                            print(f"üíæ DataFrame —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {df_path}")

                    else:
                        current_step_result_str = f"Error: Unknown tool '{tool_name}'"
                        print("‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç.")

                    # Store raw output for cumulative judging
                    cumulative_outputs.append(
                        f"Step {step_count}: {tool_name}\nResult:\n{current_step_result_str}"
                    )

                except Exception as e:
                    err_msg = f"Error executing {tool_name}: {e}"
                    print(f"‚ùå {err_msg}")
                    current_step_result_str = err_msg

                # 3. Update Long-Term Memory
                print("   üß† –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏...")
                memory = llm.update_memory(memory, tool_name, current_step_result_str)

                # Save memory trace
                with open(
                    os.path.join(output_dir, "memory.md"), "w", encoding="utf-8"
                ) as f:
                    f.write(memory)

                # 4. Global Cumulative Step Evaluation
                print("\n‚öñÔ∏è –ì–õ–û–ë–ê–õ–¨–ù–ê–Ø –û–¶–ï–ù–ö–ê –ü–†–û–ì–†–ï–°–°–ê –°–ï–°–°–ò–ò...")
                cumulative_context = "\n---\n".join(cumulative_outputs)
                judge_verdict = llm.judge_cumulative_progress(
                    memory, len(artifacts), cumulative_context
                )
                global_passed = judge_verdict.get("passed", False)
                critique = judge_verdict.get("critique", "–ù–µ—Ç –∑–∞–º–µ—á–∞–Ω–∏–π")

                if not global_passed:
                    print(f"‚ùå –ü–†–û–ì–†–ï–°–° –°–ï–°–°–ò–ò –ü–†–ò–ó–ù–ê–ù –ù–ï–£–°–ü–ï–®–ù–´–ú. –ö—Ä–∏—Ç–∏–∫–∞: {critique}")
                    if session_attempt < MAX_SESSION_RETRIES:
                        print("üîÑ –ü–ï–†–ï–ó–ê–ü–£–°–ö –°–ï–°–°–ò–ò –° –£–ß–ï–¢–û–ú –ö–†–ò–¢–ò–ö–ò...")
                        # Inject critique into memory for next run
                        memory = f"Previous Session Failed.\nCritique: {critique}\nRestarting process...\nData loaded."
                        current_df = df.copy()  # Reset DF
                        artifacts = {}  # Reset artifacts
                        # Break out of step loop to restart session
                        break
                    else:
                        print("‚õî –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ —Å–µ—Å—Å–∏–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã.")
                        # Even if failed, we break to exit the analysis phase
                        break
                else:
                    print("‚úÖ –ü—Ä–æ–≥—Ä–µ—Å—Å —Å–µ—Å—Å–∏–∏ –æ–¥–æ–±—Ä–µ–Ω –°—É–¥—å–µ–π.")

                if step_count >= MAX_STEPS:
                    print("‚ö†Ô∏è –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç —à–∞–≥–æ–≤ –∞–≥–µ–Ω—Ç–∞ –≤–Ω—É—Ç—Ä–∏ —Å–µ—Å—Å–∏–∏.")
                    global_passed = True  # Allow moving to chat if we hit step limit but judge was ok with progress so far
                    break

    if global_passed:
        # Calculate and print total time
        total_time = time.time() - start_time
        minutes = int(total_time // 60)
        seconds = int(total_time % 60)
        time_str = f"{minutes} –º–∏–Ω {seconds} —Å–µ–∫" if minutes > 0 else f"{seconds} —Å–µ–∫"
        print(f"\n‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {time_str}")

        # --- Interactive QA Mode ---
        print("\n=========================================")
        print("üí¨ –†–ï–ñ–ò–ú –ö–û–ù–°–£–õ–¨–¢–ê–¶–ò–ò")
        print(
            "–¢–µ–ø–µ—Ä—å –≤—ã –º–æ–∂–µ—Ç–µ –∑–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã –ø–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É. –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è."
        )
        print("–í–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å –∏–ª–∏ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞.")
        print("=========================================\n")

        # Ensure chat_history is loaded (if resumed) or empty
        # chat_history is initialized at start of main()

        while True:
            try:
                user_input = safe_input("\nüë§ –í–∞—à –≤–æ–ø—Ä–æ—Å: ")
                if user_input.lower() in ["exit", "quit", "–≤—ã—Ö–æ–¥"]:
                    print("üèÅ –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã. –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                    print_used_tools(used_tools)
                    break

                if not user_input:
                    continue

                print("ü§ñ –ê–≥–µ–Ω—Ç –¥—É–º–∞–µ—Ç...")

                # Context Window Management (Sliding Window)
                MAX_CHAT_CONTEXT = 20  # Keep last 20 messages
                recent_history = (
                    chat_history[-MAX_CHAT_CONTEXT:]
                    if len(chat_history) > MAX_CHAT_CONTEXT
                    else chat_history
                )

                # Convert history to string
                history_str = "\n".join(
                    [f"{msg['role']}: {msg['content']}" for msg in recent_history]
                )

                # Read Knowledge Base
                kb_path = os.path.join(output_dir, "knowledge_base.md")
                knowledge_base_content = ""
                if os.path.exists(kb_path):
                    with open(kb_path, "r", encoding="utf-8") as f:
                        knowledge_base_content = f.read()

                # Get Answer with potential Tool Use
                tools_desc = get_tools_description() if current_df is not None else ""
                response_data = llm.answer_user_question(
                    memory,
                    final_report_content,
                    history_str,
                    user_input,
                    knowledge_base_content,
                    tools_desc,
                )

                answer_text = None

                # Check if tool use is requested
                tool_call = response_data.get("tool_call")
                needs_code = response_data.get("needs_code", False)

                if tool_call and current_df is not None:
                    tool_name = tool_call.get("name")
                    tool_args = tool_call.get("args", {})
                    used_tools.add(tool_name)
                    print(
                        f"üîß –í—ã–∑–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞: {tool_name} (–∞—Ä–≥—É–º–µ–Ω—Ç—ã: {json.dumps(tool_args, ensure_ascii=False)})"
                    )

                    # PROACTIVE ROUTER: If agent asks for complex analysis, skip direct execution and go to code
                    if tool_name == "run_complex_analysis":
                        print(
                            "üöÄ –ê–≥–µ–Ω—Ç –≤—ã–±—Ä–∞–ª 'run_complex_analysis' ‚Äî —Å—Ä–∞–∑—É –∑–∞–ø—É—Å–∫–∞–µ–º Code Interpreter –¥–ª—è —Å–ª–æ–∂–Ω–æ–π –∑–∞–¥–∞—á–∏."
                        )
                        needs_code = True
                        tool_result = {"status": "skipped_for_code"}  # Dummy result
                    else:
                        tool_result = execute_tool(tool_name, tool_args, current_df)

                    if "error" in tool_result:
                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞: {tool_result['error']}")
                        print("üîÑ –ü–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ Code Interpreter...")
                        needs_code = True
                    elif needs_code:
                        # Skip verification for proactive router
                        pass
                    else:
                        tool_result_str = str(tool_result)
                        # Verify result
                        print("üîé –°–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞...")
                        verification = llm.verify_result(user_input, tool_result_str)

                        thought = verification.get("thought", "–ù–µ—Ç –æ–±—ä—è—Å–Ω–µ–Ω–∏—è.")
                        print(f"üí≠ –ú—ã—Å–ª—å –∞–≥–µ–Ω—Ç–∞: {thought}")

                        is_valid = verification.get("is_valid", True)

                        if is_valid is False:
                            print(
                                f"ü§î –°–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–∞: –æ—Ç–≤–µ—Ç –∫–∞–∂–µ—Ç—Å—è –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º. {verification.get('critique')}"
                            )
                            print(f"üí° –°–æ–≤–µ—Ç: {verification.get('suggestion')}")
                            print(
                                "üîÑ –†–µ–∑—É–ª—å—Ç–∞—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –Ω–µ —É—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç. –ü–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ Code Interpreter –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è..."
                            )
                            needs_code = True
                            answer_text = (
                                None  # Reset answer so it drops through to CI block
                            )
                        else:
                            if is_valid == "partial":
                                print(
                                    f"‚ö†Ô∏è –û—Ç–≤–µ—Ç –ø—Ä–∏–Ω—è—Ç —Å –æ–≥–æ–≤–æ—Ä–∫–∞–º–∏ (Partial Success): {verification.get('critique')}"
                                )

                            print(
                                f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞: {json.dumps(tool_result, ensure_ascii=False, indent=2)}"
                            )
                            followup_data = llm.interpret_tool_result(
                                user_input, tool_result
                            )
                            answer_text = followup_data.get("answer", str(tool_result))

                            # DOUBLE SAFETY: If answer admits failure, force fallback
                            failure_triggers = [
                                "–Ω–µ –≤–∏–∂—É",
                                "–Ω–µ—Ç –Ω—É–∂–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö",
                                "–≤–µ—Ä–Ω—É–ª —Ç–æ–ª—å–∫–æ",
                            ]
                            if any(
                                trigger in answer_text.lower()
                                for trigger in failure_triggers
                            ):
                                print(
                                    f"üö® –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ç–æ—Ä –æ–±–Ω–∞—Ä—É–∂–∏–ª –Ω–µ—Ö–≤–∞—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö: '{answer_text}'"
                                )
                                print(
                                    "üîÑ –ü–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ Code Interpreter –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è..."
                                )
                                needs_code = True
                                answer_text = None

                # Code Interpreter fallback (dynamic pandas execution)
                if (needs_code or answer_text is None) and current_df is not None:
                    # Check if this looks like a calculation question
                    calc_keywords = [
                        "—Å–∫–æ–ª—å–∫–æ",
                        "–∫–∞–∫–æ–π –ø—Ä–æ—Ü–µ–Ω—Ç",
                        "–ø–æ—Å—á–∏—Ç–∞–π",
                        "–≤—ã—á–∏—Å–ª–∏",
                        "–Ω–∞–π–¥–∏",
                        "–ø–æ–∫–∞–∂–∏",
                        "–ø–æ–¥—Å—á–∏—Ç–∞–π",
                        "—Å—Ä–µ–¥–Ω–µ–µ",
                        "–º–µ–¥–∏–∞–Ω–∞",
                        "—Ç–æ–ø",
                        "—Ä–µ–¥–∫–∏–π",
                        "—á–∞—Å—Ç—ã–π",
                    ]

                    # Force checks if explicit tool failure requested code
                    is_calc_question = (
                        any(kw in user_input.lower() for kw in calc_keywords)
                        or needs_code
                    )

                    if is_calc_question:
                        print("üß† –ó–∞–ø—É—Å–∫ Code Interpreter...")
                        used_tools.add("Code Interpreter")
                        df_info = get_df_info_for_llm(current_df)

                        MAX_CODE_ATTEMPTS = 3
                        previous_error = ""

                        for attempt in range(MAX_CODE_ATTEMPTS):
                            # Generate code
                            context = {
                                "knowledge_base": knowledge_base_content,
                                "memory": memory,
                                "final_report": final_report_content,
                            }
                            code_response = llm.generate_pandas_code(
                                user_input, df_info, previous_error, context=context
                            )
                            thought = code_response.get("thought", "")
                            code = code_response.get("code", "")

                            print(f"üí≠ –ú—ã—Å–ª—å: {thought}")
                            print(f"üìù –ö–æ–¥:\n```python\n{code}\n```")

                            # 1. Validate Code Syntax
                            validation = validate_code_syntax(code)
                            if not validation["success"]:
                                print(
                                    f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–∞ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {validation['error']}"
                                )
                                previous_error = validation["error"]
                                if attempt == MAX_CODE_ATTEMPTS - 1:
                                    answer_text = f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –∫–æ–¥. –û—à–∏–±–∫–∞: {previous_error}"
                                continue

                            # 2. User Confirmation
                            print(
                                "\n‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ê–≥–µ–Ω—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª –∫–æ–¥ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è."
                            )
                            confirm = safe_input(
                                "–ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏–ª–∏ –ª—é–±–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–º–µ–Ω—ã: "
                            )
                            if confirm:
                                print("üö´ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ—Ç–º–µ–Ω–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.")
                                answer_text = (
                                    "–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–¥–∞ –±—ã–ª–æ –æ—Ç–º–µ–Ω–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º."
                                )
                                break

                            # 3. Execute code
                            exec_result = execute_pandas_code(code, current_df)

                            if exec_result["success"]:
                                print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç: {exec_result['result']}")

                                # VERIFY Code Result
                                verification = llm.verify_result(
                                    user_input, exec_result["result"]
                                )
                                if verification.get("is_valid", True) is False:
                                    print(
                                        f"ü§î –°–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–¥–∞: {verification.get('critique')}"
                                    )
                                    previous_error = (
                                        f"–†–µ–∑—É–ª—å—Ç–∞—Ç –±—ã–ª –ø–æ–ª—É—á–µ–Ω, –Ω–æ –æ–Ω –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω: {verification.get('critique')}. "
                                        f"{verification.get('suggestion')}"
                                    )
                                    # Fallback: Store result just in case we run out of retries
                                    if attempt == MAX_CODE_ATTEMPTS - 1:
                                        answer_text = "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –º–Ω–µ –Ω–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å —Ä–∞—Å—á–µ—Ç —Ç–æ—á–Ω–æ —Å –ø–æ–º–æ—â—å—é –∫–æ–¥–∞. –Ø –ø–æ—Å—Ç–∞—Ä–∞—é—Å—å –æ—Ç–≤–µ—Ç–∏—Ç—å, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –∏–º–µ—é—â–∏—Ö—Å—è —É –º–µ–Ω—è –æ–±—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –æ –ø—Ä–æ—Ü–µ—Å—Å–µ."
                                        break
                                    continue  # Retry loop

                                # Interpret result
                                interp = llm.interpret_code_result(
                                    user_input,
                                    exec_result["result"],
                                    exec_result["result_type"],
                                )
                                answer_text = interp.get(
                                    "answer", exec_result["result"]
                                )
                                break
                            else:
                                previous_error = exec_result["error"]
                                print(f"‚ùå –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{MAX_CODE_ATTEMPTS}): {previous_error}")
                                if attempt == MAX_CODE_ATTEMPTS - 1:
                                    answer_text = "–ü—Ä–æ—Å—Ç–∏—Ç–µ, –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ —Ä–∞—Å—á–µ—Ç–æ–≤ –≤–æ–∑–Ω–∏–∫–ª–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å. –Ø –ø–æ—Å—Ç–∞—Ä–∞—é—Å—å –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–æ–¥–∞."

                # Fallback to direct answer
                if answer_text is None:
                    answer_text = response_data.get(
                        "answer", "–ù–µ –º–æ–≥—É –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å."
                    )

                knowledge_update = response_data.get("knowledge_update")

                print(f"üëâ –û—Ç–≤–µ—Ç:\n{answer_text}")

                # Process Knowledge Update
                if knowledge_update:
                    print(f"\nüìù –ê–≥–µ–Ω—Ç —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–æ–≤–æ–µ –∑–Ω–∞–Ω–∏–µ: {knowledge_update}")
                    with open(kb_path, "a", encoding="utf-8") as f:
                        f.write(f"\n- **User Insight**: {knowledge_update}")
                    # In-memory update for this loop iteration
                    knowledge_base_content += (
                        f"\n- **User Insight**: {knowledge_update}"
                    )

                # Update History
                chat_history.append({"role": "User", "content": user_input})
                chat_history.append({"role": "Assistant", "content": answer_text})

                # Persist Chat History (JSON)
                with open(
                    os.path.join(output_dir, "chat_history.json"), "w", encoding="utf-8"
                ) as f:
                    json.dump(chat_history, f, ensure_ascii=False, indent=2)

                # Persist Chat Log (Markdown)
                with open(
                    os.path.join(output_dir, "chat_log.md"), "a", encoding="utf-8"
                ) as f:
                    f.write(
                        f"**User**: {user_input}\n\n**Assistant**: {answer_text}\n\n---\n\n"
                    )

            except KeyboardInterrupt:
                print("\nüèÅ –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.")
                print_used_tools(used_tools)
                break
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —á–∞—Ç–µ: {e}")

    print_used_tools(used_tools)


if __name__ == "__main__":
    main()
